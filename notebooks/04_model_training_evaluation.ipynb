{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Notebook 04 — Model Training & Evaluation\n\nTrain a Random Forest classifier on extracted features, evaluate with cross-validation,\nand produce detailed metrics.\n\n**Classifier:** Random Forest"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys, os\nsys.path.insert(0, os.path.abspath('../src'))\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import (\n    classification_report, confusion_matrix, accuracy_score,\n    f1_score, precision_score, recall_score\n)\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom visualization import plot_confusion_matrix\n\nsns.set_theme(style='whitegrid')\n%matplotlib inline"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Extracted Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XPQRS features: (17000, 37)\n",
      "PQ Disturbances features: (798, 73)\n"
     ]
    }
   ],
   "source": [
    "# Load XPQRS features (from Notebook 03)\n",
    "xpqrs_df = pd.read_csv('../results/tables/xpqrs_features.csv')\n",
    "print(f'XPQRS features: {xpqrs_df.shape}')\n",
    "\n",
    "# Load PQ Disturbances features\n",
    "pq_df = pd.read_csv('../results/tables/pq_features.csv')\n",
    "print(f'PQ Disturbances features: {pq_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Training Pipeline"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_and_evaluate(df, dataset_name):\n    \"\"\"Train Random Forest classifier and return results.\"\"\"\n    # Prepare data\n    le = LabelEncoder()\n    feature_cols = [c for c in df.columns if c != 'label']\n    X = df[feature_cols].values\n    y = le.fit_transform(df['label'])\n    class_names = le.classes_\n\n    # Replace inf/nan\n    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n\n    # Stratified train-test split (80/20)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42, stratify=y\n    )\n    print(f'\\n{\"=\"*60}')\n    print(f'Dataset: {dataset_name}')\n    print(f'Train: {X_train.shape}, Test: {X_test.shape}')\n    print(f'Classes: {len(class_names)}')\n    print(f'{\"=\"*60}')\n\n    clf = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42, n_jobs=-1)\n\n    # Build pipeline\n    pipe = Pipeline([\n        ('scaler', StandardScaler()),\n        ('clf', clf)\n    ])\n\n    # 5-fold cross-validation\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    cv_results = cross_validate(\n        pipe, X_train, y_train, cv=cv,\n        scoring=['accuracy', 'f1_macro'],\n        return_train_score=False, n_jobs=-1\n    )\n\n    cv_acc = cv_results['test_accuracy']\n    cv_f1  = cv_results['test_f1_macro']\n    print(f'\\n--- Random Forest ---')\n    print(f'  CV Accuracy: {cv_acc.mean():.4f} (+/- {cv_acc.std():.4f})')\n    print(f'  CV F1 Macro: {cv_f1.mean():.4f} (+/- {cv_f1.std():.4f})')\n\n    # Train on full training set, evaluate on test set\n    pipe.fit(X_train, y_train)\n    y_pred = pipe.predict(X_test)\n\n    acc = accuracy_score(y_test, y_pred)\n    f1  = f1_score(y_test, y_pred, average='macro')\n    prec = precision_score(y_test, y_pred, average='macro')\n    rec  = recall_score(y_test, y_pred, average='macro')\n\n    print(f'  Test Accuracy : {acc:.4f}')\n    print(f'  Test F1 Macro : {f1:.4f}')\n    print(f'  Test Precision: {prec:.4f}')\n    print(f'  Test Recall   : {rec:.4f}')\n\n    results = {\n        'pipeline': pipe,\n        'accuracy': acc,\n        'f1_macro': f1,\n        'precision_macro': prec,\n        'recall_macro': rec,\n        'cv_accuracy_mean': cv_acc.mean(),\n        'cv_accuracy_std': cv_acc.std(),\n        'cv_f1_mean': cv_f1.mean(),\n        'cv_f1_std': cv_f1.std(),\n        'y_test': y_test,\n        'y_pred': y_pred,\n        'class_names': class_names,\n    }\n\n    # Save model\n    model_path = f'../results/models/{dataset_name}_random_forest.pkl'\n    joblib.dump(pipe, model_path)\n\n    return results, le, class_names"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train on XPQRS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%time\nxpqrs_results, xpqrs_le, xpqrs_classes = train_and_evaluate(xpqrs_df, 'xpqrs')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Results summary table\nres = xpqrs_results\nxpqrs_summary = pd.DataFrame([{\n    'Model': 'Random Forest',\n    'CV Accuracy': f\"{res['cv_accuracy_mean']:.4f} +/- {res['cv_accuracy_std']:.4f}\",\n    'Test Accuracy': f\"{res['accuracy']:.4f}\",\n    'Test F1 (Macro)': f\"{res['f1_macro']:.4f}\",\n    'Test Precision': f\"{res['precision_macro']:.4f}\",\n    'Test Recall': f\"{res['recall_macro']:.4f}\",\n}])\nxpqrs_summary.to_csv('../results/tables/xpqrs_model_results.csv', index=False)\nxpqrs_summary"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Confusion matrix\nfig = plot_confusion_matrix(\n    xpqrs_results['y_test'], xpqrs_results['y_pred'], xpqrs_classes,\n    title=f'XPQRS — Random Forest (Accuracy: {xpqrs_results[\"accuracy\"]:.4f})',\n    figsize=(14, 12)\n)\nfig.savefig('../results/figures/xpqrs_cm_random_forest.png', dpi=150, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Detailed classification report\nprint(f'Random Forest (Accuracy: {xpqrs_results[\"accuracy\"]:.4f})\\n')\nprint(classification_report(xpqrs_results['y_test'], xpqrs_results['y_pred'],\n                            target_names=xpqrs_classes))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train on PQ Disturbances Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pq_results, pq_le, pq_classes = train_and_evaluate(pq_df, 'pq_disturbances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Results summary table\nres = pq_results\npq_summary = pd.DataFrame([{\n    'Model': 'Random Forest',\n    'CV Accuracy': f\"{res['cv_accuracy_mean']:.4f} +/- {res['cv_accuracy_std']:.4f}\",\n    'Test Accuracy': f\"{res['accuracy']:.4f}\",\n    'Test F1 (Macro)': f\"{res['f1_macro']:.4f}\",\n    'Test Precision': f\"{res['precision_macro']:.4f}\",\n    'Test Recall': f\"{res['recall_macro']:.4f}\",\n}])\npq_summary.to_csv('../results/tables/pq_model_results.csv', index=False)\npq_summary"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Confusion matrix\nfig = plot_confusion_matrix(\n    pq_results['y_test'], pq_results['y_pred'], pq_classes,\n    title=f'PQ Disturbances — Random Forest (Accuracy: {pq_results[\"accuracy\"]:.4f})',\n    figsize=(12, 10)\n)\nfig.savefig('../results/figures/pq_cm_random_forest.png', dpi=150, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Detailed classification report\nprint(f'Random Forest (Accuracy: {pq_results[\"accuracy\"]:.4f})\\n')\nprint(classification_report(pq_results['y_test'], pq_results['y_pred'],\n                            target_names=pq_classes))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Next:** [05_results_comparison.ipynb](05_results_comparison.ipynb) — Cross-dataset comparison and detailed analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}