{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 05 — Results Comparison & Analysis\n",
    "\n",
    "Comprehensive visualization and cross-dataset comparison for the final report."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath('../src'))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, f1_score,\n",
    "    roc_curve, auc\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from visualization import plot_confusion_matrix, plot_pca_2d, plot_feature_importance\n",
    "from feature_extractor import get_feature_domain, ALL_FEATURE_NAMES\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "%matplotlib inline"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "xpqrs_summary = pd.read_csv('../results/tables/xpqrs_model_results.csv')\n",
    "pq_summary    = pd.read_csv('../results/tables/pq_model_results.csv')\n",
    "\n",
    "print('XPQRS Results:')\n",
    "display(xpqrs_summary)\n",
    "print('\\nPQ Disturbances Results:')\n",
    "display(pq_summary)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cross-Dataset Accuracy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Merge results for side-by-side comparison\n",
    "xpqrs_acc = xpqrs_summary[['Model', 'Test Accuracy']].rename(columns={'Test Accuracy': 'XPQRS'})\n",
    "pq_acc = pq_summary[['Model', 'Test Accuracy']].rename(columns={'Test Accuracy': 'PQ Disturbances'})\n",
    "comparison = xpqrs_acc.merge(pq_acc, on='Model')\n",
    "\n",
    "# Convert to float for plotting\n",
    "comparison['XPQRS'] = comparison['XPQRS'].astype(float)\n",
    "comparison['PQ Disturbances'] = comparison['PQ Disturbances'].astype(float)\n",
    "comparison = comparison.sort_values('XPQRS', ascending=False)\n",
    "\n",
    "display(comparison)\n",
    "\n",
    "# Grouped bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(comparison))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, comparison['XPQRS'], width, label='XPQRS (17 classes)', color='#2196F3')\n",
    "bars2 = ax.bar(x + width/2, comparison['PQ Disturbances'], width, label='PQ Disturbances (13 classes)', color='#FF9800')\n",
    "\n",
    "ax.set_ylabel('Test Accuracy')\n",
    "ax.set_title('Cross-Dataset Model Comparison', fontweight='bold', fontsize=13)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison['Model'], rotation=20, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.1)\n",
    "\n",
    "for bar in bars1:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "            f'{bar.get_height():.3f}', ha='center', va='bottom', fontsize=8)\n",
    "for bar in bars2:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "            f'{bar.get_height():.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig('../results/figures/cross_dataset_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Per-Class F1 Score Heatmap (XPQRS)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load features and re-run predictions to get per-class metrics\n",
    "xpqrs_df = pd.read_csv('../results/tables/xpqrs_features.csv')\n",
    "feature_cols = [c for c in xpqrs_df.columns if c != 'label']\n",
    "\n",
    "le = LabelEncoder()\n",
    "X = np.nan_to_num(xpqrs_df[feature_cols].values, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "y = le.fit_transform(xpqrs_df['label'])\n",
    "class_names = le.classes_\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Load saved models and compute per-class F1\n",
    "model_names = ['SVM', 'Random Forest', 'Decision Tree', 'KNN', 'Logistic Regression', 'Gradient Boosting']\n",
    "f1_matrix = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    model_file = f'../results/models/xpqrs_{model_name.replace(\" \", \"_\").lower()}.pkl'\n",
    "    pipe = joblib.load(model_file)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    f1_per_class = f1_score(y_test, y_pred, average=None)\n",
    "    f1_matrix.append(f1_per_class)\n",
    "\n",
    "f1_df = pd.DataFrame(f1_matrix, index=model_names, columns=class_names)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 6))\n",
    "sns.heatmap(f1_df, annot=True, fmt='.2f', cmap='YlOrRd', ax=ax, linewidths=0.5,\n",
    "            vmin=0, vmax=1)\n",
    "ax.set_title('Per-Class F1 Score — XPQRS Dataset', fontweight='bold', fontsize=13)\n",
    "ax.set_ylabel('Classifier')\n",
    "ax.set_xlabel('Disturbance Class')\n",
    "ax.tick_params(labelsize=8)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "fig.savefig('../results/figures/xpqrs_f1_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load Random Forest model for feature importance\n",
    "rf_pipe = joblib.load('../results/models/xpqrs_random_forest.pkl')\n",
    "rf_model = rf_pipe.named_steps['clf']\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "fig = plot_feature_importance(importances, feature_cols, top_n=25,\n",
    "                              title='Top 25 Feature Importances (Random Forest — XPQRS)')\n",
    "fig.savefig('../results/figures/final_feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Feature importance by domain\n",
    "domain_importance = {'time': 0, 'fft': 0, 'wavelet': 0}\n",
    "domain_counts = {'time': 0, 'fft': 0, 'wavelet': 0}\n",
    "\n",
    "for feat, imp in zip(feature_cols, importances):\n",
    "    domain = get_feature_domain(feat)\n",
    "    if domain in domain_importance:\n",
    "        domain_importance[domain] += imp\n",
    "        domain_counts[domain] += 1\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Total importance by domain\n",
    "domains = list(domain_importance.keys())\n",
    "totals = [domain_importance[d] for d in domains]\n",
    "colors = ['#2196F3', '#4CAF50', '#FF9800']\n",
    "axes[0].bar(domains, totals, color=colors)\n",
    "axes[0].set_title('Total Feature Importance by Domain', fontweight='bold')\n",
    "axes[0].set_ylabel('Sum of Importances')\n",
    "for bar, val in zip(axes[0].patches, totals):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "                 f'{val:.3f}', ha='center', fontsize=10)\n",
    "\n",
    "# Average importance by domain\n",
    "averages = [domain_importance[d] / max(domain_counts[d], 1) for d in domains]\n",
    "axes[1].bar(domains, averages, color=colors)\n",
    "axes[1].set_title('Average Feature Importance by Domain', fontweight='bold')\n",
    "axes[1].set_ylabel('Mean Importance per Feature')\n",
    "for bar, val in zip(axes[1].patches, averages):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
    "                 f'{val:.4f}', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig('../results/figures/importance_by_domain.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. t-SNE Visualization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# t-SNE on test set (faster than full dataset)\nscaler = StandardScaler()\nX_test_scaled = scaler.fit_transform(X_test)\n\ntsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000)\nX_tsne = tsne.fit_transform(X_test_scaled)\n\ntest_labels = le.inverse_transform(y_test)\nfig = plot_pca_2d(X_tsne, test_labels, sorted(class_names),\n                  title='t-SNE — XPQRS Feature Space (Test Set)')\nfig.savefig('../results/figures/tsne_xpqrs.png', dpi=150, bbox_inches='tight')\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Misclassification Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Use the best model\n",
    "best_model_name = xpqrs_summary.iloc[0]['Model']\n",
    "best_pipe = joblib.load(f'../results/models/xpqrs_{best_model_name.replace(\" \", \"_\").lower()}.pkl')\n",
    "y_pred_best = best_pipe.predict(X_test)\n",
    "\n",
    "# Find misclassified samples\n",
    "misclassified = y_test != y_pred_best\n",
    "n_misclassified = misclassified.sum()\n",
    "total = len(y_test)\n",
    "print(f'Best model: {best_model_name}')\n",
    "print(f'Misclassified: {n_misclassified}/{total} ({100*n_misclassified/total:.2f}%)')\n",
    "\n",
    "# Most confused class pairs\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "np.fill_diagonal(cm, 0)  # zero out diagonal\n",
    "\n",
    "# Top 10 confused pairs\n",
    "confused_pairs = []\n",
    "for i in range(len(class_names)):\n",
    "    for j in range(len(class_names)):\n",
    "        if cm[i, j] > 0:\n",
    "            confused_pairs.append((class_names[i], class_names[j], cm[i, j]))\n",
    "\n",
    "confused_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "print(f'\\nTop 10 most confused class pairs:')\n",
    "for true_cls, pred_cls, count in confused_pairs[:10]:\n",
    "    print(f'  {true_cls:35s} -> {pred_cls:35s}  ({count} samples)')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ROC Curves (Best Model, One-vs-Rest)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from sklearn.preprocessing import label_binarize\nfrom sklearn.pipeline import Pipeline\n\n# Use Logistic Regression for probability estimates (needed for ROC curves)\nfrom sklearn.linear_model import LogisticRegression\n\nroc_pipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('clf', LogisticRegression(C=1.0, solver='lbfgs', max_iter=2000, random_state=42))\n])\nroc_pipe.fit(X_train, y_train)\ny_prob = roc_pipe.predict_proba(X_test)\n\n# Binarize labels\ny_test_bin = label_binarize(y_test, classes=np.arange(len(class_names)))\n\n# Plot ROC for all classes\nfig, ax = plt.subplots(figsize=(10, 8))\ncolors = plt.cm.tab20(np.linspace(0, 1, len(class_names)))\n\nfor i, (cls, color) in enumerate(zip(class_names, colors)):\n    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_prob[:, i])\n    roc_auc = auc(fpr, tpr)\n    ax.plot(fpr, tpr, color=color, linewidth=1.0, alpha=0.8,\n            label=f'{cls} (AUC={roc_auc:.3f})')\n\nax.plot([0, 1], [0, 1], 'k--', linewidth=0.5)\nax.set_xlabel('False Positive Rate')\nax.set_ylabel('True Positive Rate')\nax.set_title('ROC Curves — One-vs-Rest (Logistic Regression)', fontweight='bold')\nax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=7)\nplt.tight_layout()\nfig.savefig('../results/figures/roc_curves.png', dpi=150, bbox_inches='tight')\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Combined summary for report\nfinal_table = comparison.copy()\n\n# Add F1 scores\nxpqrs_f1 = xpqrs_summary[['Model', 'Test F1 (Macro)']].rename(columns={'Test F1 (Macro)': 'XPQRS F1'})\npq_f1 = pq_summary[['Model', 'Test F1 (Macro)']].rename(columns={'Test F1 (Macro)': 'PQ F1'})\nfinal_table = final_table.merge(xpqrs_f1, on='Model').merge(pq_f1, on='Model')\n\nfinal_table = final_table.rename(columns={'XPQRS': 'XPQRS Accuracy', 'PQ Disturbances': 'PQ Accuracy'})\nfinal_table = final_table.sort_values('XPQRS Accuracy', ascending=False)\n\nprint('\\n=== FINAL RESULTS SUMMARY ===')\nprint(f'XPQRS: 17 classes, 17000 signals, 36 extracted features')\nprint(f'PQ Disturbances: 13 classes, ~798 signals, 72 pre-extracted wavelet features')\nprint()\ndisplay(final_table)\n\nfinal_table.to_csv('../results/tables/final_comparison.csv', index=False)\nprint('\\nSaved to: results/tables/final_comparison.csv')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "1. **Best performing classifier** on XPQRS dataset: (see table above)\n",
    "2. **Feature domain contribution**: The relative importance of time-domain, FFT, and wavelet features\n",
    "3. **Most confused classes**: Compound disturbances (e.g., Harmonics_with_Sag vs. Sag_with_Harmonics) are hardest to distinguish\n",
    "4. **Cross-dataset comparison**: PQ Disturbances dataset has fewer samples but benefits from pre-extracted wavelet features\n",
    "5. **Multi-domain feature extraction** (time + FFT + wavelet) provides comprehensive signal characterization for traditional ML classifiers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}