{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Notebook 03 — Feature Extraction\n\nExtract engineered features from raw XPQRS signals for use with traditional ML classifiers.\n\n**Feature categories:**\n| Domain | Count | Examples |\n|---|---|---|\n| Time-domain | 14 | RMS, Crest Factor, Kurtosis, Zero-Crossing Rate |\n| Frequency (FFT) | 10 | Fundamental magnitude, THD, Spectral Centroid |\n| Wavelet (DWT) | 12 | Sub-band energies, std, entropy (db4, 3 levels) |\n| **Total** | **36** | |"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath('../src'))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from data_loader import load_xpqrs, load_pq_disturbances_as_dataframe\n",
    "from feature_extractor import (\n",
    "    extract_all_features, extract_features_batch,\n",
    "    TIME_FEATURE_NAMES, FFT_FEATURE_NAMES, WAVELET_FEATURE_NAMES,\n",
    "    ALL_FEATURE_NAMES, get_feature_domain\n",
    ")\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "XPQRS_DIR = '../dataset/XPQRS/'\n",
    "PQ_DIR    = '../dataset/PQ Disturbances Dataset/'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Feature Extraction — XPQRS Dataset\n\nExtract 36 features from each of 17,000 raw waveform signals."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "signals, labels = load_xpqrs(XPQRS_DIR)\n",
    "print(f'Signals: {signals.shape}, Labels: {labels.shape}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Demo: extract features from one signal\n",
    "demo_features = extract_all_features(signals[0])\n",
    "print(f'Features per signal: {len(demo_features)}')\n",
    "for name, val in demo_features.items():\n",
    "    print(f'  {name:25s} = {val:.6f}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%time\n",
    "# Extract features from ALL signals (takes ~1-2 minutes)\n",
    "xpqrs_features_df = extract_features_batch(signals, labels, verbose=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(f'Feature matrix shape: {xpqrs_features_df.shape}')\n",
    "print(f'Columns: {list(xpqrs_features_df.columns)}')\n",
    "xpqrs_features_df.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Check for NaN or Inf\n",
    "feature_cols = [c for c in xpqrs_features_df.columns if c != 'label']\n",
    "print(f'NaN count: {xpqrs_features_df[feature_cols].isna().sum().sum()}')\n",
    "print(f'Inf count: {np.isinf(xpqrs_features_df[feature_cols].values).sum()}')\n",
    "\n",
    "# Replace any Inf with NaN, then fill with 0\n",
    "xpqrs_features_df[feature_cols] = xpqrs_features_df[feature_cols].replace([np.inf, -np.inf], np.nan).fillna(0)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Correlation matrix\n",
    "corr = xpqrs_features_df[feature_cols].corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 14))\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "sns.heatmap(corr, mask=mask, cmap='coolwarm', center=0, ax=ax,\n",
    "            xticklabels=True, yticklabels=True, linewidths=0.5)\n",
    "ax.set_title('Feature Correlation Matrix', fontweight='bold')\n",
    "ax.tick_params(labelsize=6)\n",
    "plt.tight_layout()\n",
    "fig.savefig('../results/figures/feature_correlation_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Find highly correlated feature pairs (> 0.95)\n",
    "high_corr_pairs = []\n",
    "for i in range(len(feature_cols)):\n",
    "    for j in range(i+1, len(feature_cols)):\n",
    "        if abs(corr.iloc[i, j]) > 0.95:\n",
    "            high_corr_pairs.append((feature_cols[i], feature_cols[j], corr.iloc[i, j]))\n",
    "\n",
    "print(f'Highly correlated pairs (|r| > 0.95): {len(high_corr_pairs)}')\n",
    "for f1, f2, r in high_corr_pairs:\n",
    "    print(f'  {f1:25s} <-> {f2:25s}  r = {r:.3f}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Feature distribution by domain\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "domain_features = {\n",
    "    'Time-Domain': TIME_FEATURE_NAMES,\n",
    "    'FFT': FFT_FEATURE_NAMES,\n",
    "    'Wavelet': WAVELET_FEATURE_NAMES\n",
    "}\n",
    "\n",
    "for ax, (domain, feat_names) in zip(axes, domain_features.items()):\n",
    "    valid_feats = [f for f in feat_names if f in feature_cols]\n",
    "    data = xpqrs_features_df[valid_feats]\n",
    "    ax.boxplot(data.values, labels=valid_feats, vert=True)\n",
    "    ax.set_title(f'{domain} Features', fontweight='bold')\n",
    "    ax.tick_params(axis='x', rotation=45, labelsize=7)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig('../results/figures/feature_distributions_by_domain.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quick Feature Importance Preview (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(xpqrs_features_df['label'])\n",
    "X = xpqrs_features_df[feature_cols].values\n",
    "\n",
    "# Quick RF for feature importance\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_scaled, y)\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "sorted_idx = np.argsort(importances)[::-1]\n",
    "\n",
    "print('Top 15 features by importance:')\n",
    "for i in range(15):\n",
    "    idx = sorted_idx[i]\n",
    "    domain = get_feature_domain(feature_cols[idx])\n",
    "    print(f'  {i+1:2d}. {feature_cols[idx]:25s} ({domain:7s}) = {importances[idx]:.4f}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from visualization import plot_feature_importance\n",
    "\n",
    "fig = plot_feature_importance(importances, feature_cols, top_n=20,\n",
    "                              title='Top 20 Feature Importances (Random Forest)')\n",
    "fig.savefig('../results/figures/feature_importance_rf.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dimensionality Reduction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from visualization import plot_pca_2d\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(f'Explained variance: PC1={pca.explained_variance_ratio_[0]:.3f}, PC2={pca.explained_variance_ratio_[1]:.3f}')\n",
    "print(f'Total: {sum(pca.explained_variance_ratio_):.3f}')\n",
    "\n",
    "fig = plot_pca_2d(X_pca, xpqrs_features_df['label'].values,\n",
    "                  sorted(xpqrs_features_df['label'].unique()),\n",
    "                  title='PCA — XPQRS Feature Space (2D)')\n",
    "fig.savefig('../results/figures/pca_xpqrs.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Scree plot\n",
    "pca_full = PCA().fit(X_scaled)\n",
    "cumvar = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.bar(range(1, len(cumvar)+1), pca_full.explained_variance_ratio_, alpha=0.6, label='Individual')\n",
    "ax.step(range(1, len(cumvar)+1), cumvar, where='mid', color='red', label='Cumulative')\n",
    "ax.axhline(y=0.95, color='gray', linestyle='--', alpha=0.5, label='95% threshold')\n",
    "ax.set_xlabel('Principal Component')\n",
    "ax.set_ylabel('Explained Variance Ratio')\n",
    "ax.set_title('PCA Scree Plot — XPQRS Features', fontweight='bold')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "fig.savefig('../results/figures/pca_scree_plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "n_95 = np.argmax(cumvar >= 0.95) + 1\n",
    "print(f'Components needed for 95% variance: {n_95}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Extracted Features"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save XPQRS features\n",
    "xpqrs_features_df.to_csv('../results/tables/xpqrs_features.csv', index=False)\n",
    "print(f'Saved XPQRS features: {xpqrs_features_df.shape}')\n",
    "\n",
    "# Load and save PQ Disturbances features (already pre-extracted)\n",
    "pq_df = load_pq_disturbances_as_dataframe(PQ_DIR)\n",
    "pq_df.to_csv('../results/tables/pq_features.csv', index=False)\n",
    "print(f'Saved PQ Disturbances features: {pq_df.shape}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Next:** [04_model_training_evaluation.ipynb](04_model_training_evaluation.ipynb) — Train and evaluate ML classifiers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}