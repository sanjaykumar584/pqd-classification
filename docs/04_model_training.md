# 4. How the Model Gets Trained

## Overview

The model is a **Random Forest classifier** wrapped in a **scikit-learn Pipeline** with a StandardScaler. Training happens in Notebook 04 and takes about 30 seconds.

## Step-by-Step Training Process

### Step 1: Load Pre-Extracted Features

```python
xpqrs_df = pd.read_csv('results/tables/xpqrs_features.csv')
# Shape: (17000, 37) -- 36 feature columns + 1 label column
```

This CSV was generated by Notebook 03. Each row has 36 numbers (features extracted from one signal) and a string label.

### Step 2: Encode Labels

The model works with numbers, not strings. A **LabelEncoder** converts class names to integers (sorted alphabetically):

```
"Flicker"                        -> 0
"Flicker_with_Sag"               -> 1
"Flicker_with_Swell"             -> 2
"Harmonics"                      -> 3
"Harmonics_with_Sag"             -> 4
"Harmonics_with_Swell"           -> 5
"Interruption"                   -> 6
"Notch"                          -> 7
"Oscillatory_Transient"          -> 8
"Pure_Sinusoidal"                -> 9
"Sag"                            -> 10
"Sag_with_Harmonics"             -> 11
"Sag_with_Oscillatory_Transient" -> 12
"Swell"                          -> 13
"Swell_with_Harmonics"           -> 14
"Swell_with_Oscillatory_Transient" -> 15
"Transient"                      -> 16
```

### Step 3: Split Data (80/20)

```
17,000 signals
    |
    ├── 13,600 (80%) ──> Training set (model learns from these)
    └──  3,400 (20%) ──> Test set (model is evaluated on these, never seen during training)
```

The split is **stratified** — each class has the same proportion in train and test:
- Each class: 800 train + 200 test = 1000 total

### Step 4: Build the Pipeline

```python
Pipeline([
    ('scaler', StandardScaler()),      # Step A: Normalize features
    ('clf', RandomForestClassifier())   # Step B: Classify
])
```

**Step A - StandardScaler:** Each of the 36 features has a different scale (e.g., `energy` ranges 0-150, `entropy` ranges 0-5). The scaler transforms every feature to have **mean = 0** and **standard deviation = 1**:

```
scaled_value = (original_value - mean) / std
```

This ensures no single feature dominates just because it has larger numbers.

**Step B - Random Forest:** An ensemble of **100 decision trees** that vote together.

### Step 5: How Random Forest Works

A Random Forest builds many decision trees, each slightly different, and combines their predictions:

```
                    Signal Features
                   /      |       \
                Tree 1  Tree 2 ... Tree 100
                  |       |          |
                "Sag"   "Sag"    "Flicker"
                  \       |        /
                   \      |       /
                    MAJORITY VOTE
                        |
                      "Sag" (2 out of 3 trees agree)
```

**Why 100 trees?** More trees = more stable predictions, less overfitting. 100 is a good balance of accuracy and speed.

**How each tree is built:**
1. Take a random subset of training samples (with replacement — "bagging")
2. At each decision node, pick a random subset of features to consider
3. Find the best feature and threshold to split the data
4. Repeat until leaves are pure (or max depth reached)

**Example of one tree's decision path:**

```
Is peak > 1.2?
├── YES: Is thd > 0.15?
│   ├── YES: Is cD1_energy > 0.5?
│   │   ├── YES: Transient
│   │   └── NO: Harmonics
│   └── NO: Swell
└── NO: Is rms < 0.3?
    ├── YES: Interruption
    └── NO: Is fundamental_mag > 0.45?
        ├── YES: Pure_Sinusoidal
        └── NO: Sag
```

### Step 6: Cross-Validation (5-Fold)

Before the final evaluation, the model is tested using **5-fold cross-validation** on the training set to check stability:

```
Training set (13,600 samples) split into 5 folds:

Round 1: [Fold1=TEST] [Fold2=TRAIN] [Fold3=TRAIN] [Fold4=TRAIN] [Fold5=TRAIN] -> Accuracy
Round 2: [Fold1=TRAIN] [Fold2=TEST] [Fold3=TRAIN] [Fold4=TRAIN] [Fold5=TRAIN] -> Accuracy
Round 3: [Fold1=TRAIN] [Fold2=TRAIN] [Fold3=TEST] [Fold4=TRAIN] [Fold5=TRAIN] -> Accuracy
Round 4: [Fold1=TRAIN] [Fold2=TRAIN] [Fold3=TRAIN] [Fold4=TEST] [Fold5=TRAIN] -> Accuracy
Round 5: [Fold1=TRAIN] [Fold2=TRAIN] [Fold3=TRAIN] [Fold4=TRAIN] [Fold5=TEST] -> Accuracy

Average: 0.8972 +/- 0.0043
```

The low standard deviation (0.0043) means the model performs consistently, not just getting lucky on one particular split.

### Step 7: Final Training and Evaluation

After cross-validation confirms stability:
1. Train on the **full training set** (13,600 samples)
2. Predict on the **held-out test set** (3,400 samples)
3. Compute final metrics

### Step 8: Save the Model

```python
joblib.dump(pipe, 'results/models/xpqrs_random_forest.pkl')
```

The saved `.pkl` file contains:
- The fitted **StandardScaler** (with stored means and standard deviations for all 36 features)
- The trained **RandomForestClassifier** (all 100 decision trees with their rules)

This file is 44 MB because it stores 100 complete decision trees.

## Training Results

| Metric | Value | Meaning |
|---|---|---|
| **CV Accuracy** | 0.8972 +/- 0.0043 | ~90% accuracy across 5 validation folds, very stable |
| **Test Accuracy** | 0.9062 | 90.62% of test signals classified correctly |
| **Test F1 (Macro)** | 0.9056 | Balanced performance across all 17 classes |
| **Test Precision** | 0.9067 | When it predicts a class, it's right ~91% of the time |
| **Test Recall** | 0.9062 | It finds ~91% of signals belonging to each class |

## What the Model Struggles With

The confusion matrix shows that the model sometimes confuses:
- **Harmonics_with_Sag** vs **Sag_with_Harmonics** — both have harmonics + sag, differ in which is dominant
- **Harmonics_with_Swell** vs **Swell_with_Harmonics** — same issue with swell
- **Sag** vs **Flicker_with_Sag** — flicker can be subtle on top of a sag

These are compound disturbances that share many features, making them inherently harder to distinguish.

## Key Hyperparameters

| Parameter | Value | What It Controls |
|---|---|---|
| `n_estimators` | 100 | Number of trees in the forest |
| `max_depth` | None (unlimited) | Trees grow until leaves are pure |
| `criterion` | gini (default) | Split quality measure |
| `random_state` | 42 | Seed for reproducibility |
| `n_jobs` | -1 | Use all CPU cores for parallel training |
| `test_size` | 0.2 | 20% of data reserved for testing |
